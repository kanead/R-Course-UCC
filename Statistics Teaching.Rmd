---
title: "Statistics Teaching"
author: "Adam Kane"
date: "4 November 2016"
output:
  html_document: default
  pdf_document: default
---
```{r,echo=FALSE}
library(HH)
```

# Variables
A variable is something that can take on different values e.g. height is a variable. The opposite of variables are constants e.g. the gravitational constant which has one value only. 

## Types of variables (NOIR)
In statistics we can consider 4 variable types as set out by Stevens (1946):

###Nominal variables 
are variables that have two or more categories, but which do not have an intrinsic order. For example, classifying where people live in the USA by state. In this case there will be 50 'levels' of the nominal variable.

```{r}
nominalVariables <- c("Alaska", "Florida", "New York", "Washington", "Texas") 
```

###Ordinal variables 
are variables that have two or more categories just like nominal variables only the categories can also be ordered or ranked. So if you asked someone if they liked the policies of the Democratic Party and they could answer either "Not very much", "They are OK" or "Yes, a lot" then you have an ordinal variable. Why? Because you have 3 categories, namely "Not very much", "They are OK" and "Yes, a lot" and you can rank them from the most positive (Yes, a lot), to the middle response (They are OK), to the least positive (Not very much). However, whilst we can rank the levels, we cannot place a "value" to them; we cannot say that "They are OK" is twice as positive as "Not very much" for example.

```{r}
ordinalVariables <- c("OK", "Not very much", "OK", "Yes, a lot", "Not very much") 
```

###Interval variables 
are variables for which their central characteristic is that they can be measured along a continuum and they have a numerical value (for example, temperature measured in degrees Celsius or Fahrenheit). So the difference between 20C and 30C is the same as 30C to 40C. However, temperature measured in degrees Celsius or Fahrenheit is NOT a ratio variable. In interval scales, addition and subtraction make sense, but multiplication and division do not. That is, 70C is not "twice as hot"" as 35C. If this is confusing, think what a negative temperature would mean, or a 0 temperature! 30C is -1 times as hot as -30C? It doesn't make sense!

```{r}
intervalVariables <- c(30,31,29,30,29,33,34,35) 
```

###Ratio variables 
are interval variables, but with the added condition that 0 (zero) of the measurement indicates that there is none of that variable. So, temperature measured in degrees Celsius or Fahrenheit is not a ratio variable because 0C does not mean there is no temperature. However, temperature measured in Kelvin is a ratio variable as 0 Kelvin (often called absolute zero) indicates that there is no temperature whatsoever. Other examples of ratio variables include height, mass, distance and many more. Ratio responses mean that not only is there order and spacing, but that multiplication makes sense as well. Two common examples are height and weight. A person who weighs 200 pounds weighs double what a person who weighs 100 pounds weighs. 

```{r}
ratioVariables <- c(0:10) 
```

### Problematic Percentages
So, are percentages nominal, ordinal, interval or ratio? Technically, they are not even ratio - you cannot double a percentage without distorting the meaning

### Levels of measurement
In general it is advantageous to treat variables as the highest level of measurement for which they qualify.  That is, we could treat education level as a categorical variable, but usually we will want to treat it as an ordinal variable.  This is because treating it as an ordinal variable retains more of the information carried in the data.  If we were to reduce it to a categorical variable, we would lose the order of the levels of the variable.  By using a higher level of measurement, we will have more options in the way we analyze, summarize, and present data.

# Parametric Vs Non Parametric Tests 
There is a lot of confusion about parametric vs. non-parametric statistics and tests.  Some of the literature that explains the difference gets pretty technical.  Here is a layman's description that might not be 100% technically accurate but that will let you understand the difference.A parameter is a characteristic of a population.  We often estimate parameters with statistics that come from samples.  Some common parameters and statistics are the mean, the median, the standard deviation and so on.

Some tests use these parameters.  For example, every variety of the t-test uses means and standard deviations.  Therefore, the t-test is called a parametric test. On the other hand, some tests do not use these parameters.  For example, the Mann Whitney U test uses no parameters.  Therefore, it is called a non-parametric test.

If you want to tell if a test is parametric or not, look at the formulas used in calculating it.  Do they contain parameters/statistics?

If your measurement scale is nominal or ordinal then you use non-parametric statistics

If you are using interval or ratio scales you use parametric statistics.

# Histograms
A histogram is a type of graph used to display a distribution. It helps us to overcome the natural tendency to rely on summary information, such as an average. Histograms can reveal information no captured by summary statistics.


```{r, echo=FALSE}
x<-rnorm(1000, mean = 0, sd = 1)
hist(x, main = "normally distributed data")
```

```{r, echo=FALSE}
x<-rbeta(1000,2,5)
hist(x, main="positive/ right skewed data")
```

```{r, echo=FALSE}
x<-rbeta(1000,5,2)
hist(x, main="negative/ left skewed data")
```

#Summary Statistics
##Central Tendency
###The mean 
is a measure of central tendency
this describes the middle or centre point of a distribution
$mean = M = \frac{1}{n} \sum_n^{i =1} x$

###The median
is the middle score (the score below which 50% of the distribution falls)
preferred when there are extreme scores in the distribution

###The mode
is the score that occurs most often in the distribution, useful for nominal variables

###How distribution can affect measures of central tendency
Differing distribution may mean these three measures do not overlap
Here the mean is blue, the median grey and the mode red.

```{r, echo=FALSE}

x <- seq(-2.5, 10, length=1000000)
hx5 <- rnorm(x,0,1) + rexp(x,1/5) # tau=5 (rate = 1/tau)
#
# Compute the density.
#
dens <- density(hx5)
#
# Compute some measures of location.
#
n <- length(dens$y)                       #$
dx <- mean(diff(dens$x))                  # Typical spacing in x $
y.unit <- sum(dens$y) * dx                # Check: this should integrate to 1 $
dx <- dx / y.unit                         # Make a minor adjustment
x.mean <- sum(dens$y * dens$x) * dx
y.mean <- dens$y[length(dens$x[dens$x < x.mean])] #$
x.mode <- dens$x[i.mode <- which.max(dens$y)]
y.mode <- dens$y[i.mode]                  #$
y.cs <- cumsum(dens$y)                    #$
x.med <- dens$x[i.med <- length(y.cs[2*y.cs <= y.cs[n]])] #$
y.med <- dens$y[i.med]                                    #$
#
# Plot the density and the statistics.
#
plot(dens, xlim=c(-2.5,10), type="l", col="black", 
     xlab="x",ylab = "y", main="different central tendencies", lwd=2)
temp <- mapply(function(x,y,c) lines(c(x,x), c(0,y), lwd=2, col=c), 
               c(x.mean, x.med, x.mode), 
               c(y.mean, y.med, y.mode), 
               c("Blue", "Gray", "Red"))
```

##Measures of Variability
a measure that describes the range and diversity of scores in a distribution

###standard deviation (SD) 
the average deviation from the mean in a distribution

$SD = \sqrt\frac{[\Sigma(X-M^2)]}{N}$ is used for descriptive statistics

$SD = \sqrt\frac{[\Sigma(X-M^2)]}{N-1}$ is used for inferential statistics

###variance ($SD^2$)

sum of squared deviation scores = sum of squares are divided by the sample size

$SD^2 = \frac{[\Sigma(X-M^2)]}{N}$ is used for descriptive statistics

$SD^2 = \frac{[\Sigma(X-M^2)]}{N-1}$ is used for inferential statistics

this is also known as the mean squares 

###Example of Linsanity
Jeremy Lin was a basketball player who went on a scoring streak for the New York Knicks. We can calculate some summary statistics for his games.  

here are the points he scored for the games he played:

```{r}
pointsPerGame<-c(28,26,10,27,20,38,23,28,25,2)
```

we take the sum of those values and the sample size i.e. number of games he played to get the mean

```{r}
sum(pointsPerGame)
length(pointsPerGame) 
```

so the mean is 

```{r}
sum(pointsPerGame)/length(pointsPerGame)
```

then the deviation scores show how much he deviated from the mean for each game i.e. it is the difference between a raw score and the mean.

```{r}
pointsPerGame - mean(pointsPerGame)
```

we can't get the average for the deviation scores because they sum to zero 

```{r}
deviationScores<- pointsPerGame - mean(pointsPerGame)
devsum <- sum(deviationScores)
```
```{r, echo=FALSE}
devsum <- if(all.equal(0L, devsum)) 0L else devsum ;devsum
```
```{r}
devsum/length(pointsPerGame)
```

instead we square the deviation scores, sum them and divide by N to give us a score for variance. 

That is to say we calculate mean squares because it is the sums of squares divided by N. 

```{r}
(pointsPerGame - mean(pointsPerGame))^2
devSq <- (pointsPerGame - mean(pointsPerGame))^2
devSumSq <- sum(devSq) ; devSumSq
variance <- devSumSq/length(pointsPerGame) ; variance
```

Squaring however does have a problem as a measure of spread and that is that the units are all squared, where as we'd might prefer the spread to be in the same units as the original data (think of squared points scored). Hence the square root allows us to return to the original units which is the standard deviation.

```{r}
sqrt(variance)
```

# Standardised Scales
### Z-scores
In statistics there is a standard scale the Z scale.
Any score from any scale can be converted to Z scores 

$Z = \frac{(X-M)}{SD}$

X = raw score, the score on the original scale

M = mean

SD = standard deviation

The mean Z-score is Z = 0

Positive Z scores are above average

Negative Z scores are below average

For example  
```{r}
X = 99.6 # body temp for one person
M = 98.6 # the mean for the group
SD = 0.5 # the standard deviation for the group
Z=(X-M)/SD; Z
```
This value of 2 means their score is 2 standard deviations above the mean

### Percentile rank
The percentage of scores that fall at or below a score in a distribution 
Assume a normal distribution
If Z = 0 then the percentile rank = 50th 
50% of the distribution falls below the mean 




#Correlation
A statistical procedure used to measure and describe the relationship between two variables 

Correlations can range between +1 and -1

+1 is perfect positive correlation

0 is no correlation (independence)

-1 is perfect negative correlation

When two variables, let's call them X and Y, are correlated, then one variable can be used to predict the other variable.

More precisely, a person's score on X can be used to predict his or her score on Y. 

For example, working memory capacity is strongly correlated with intelligence, or IQ, in healthy young adults

So if we know a person's IQ then we can predict how they will do on a a test of working memeory. 

We can see in this scatterplot that there is a positive correlation in our data, which is verified by the value we get for our correlation.

```{r}
IQ <- rnorm(250, mean = 100, sd = 10)
workingMemory <- IQ*rnorm(250, mean = 3, sd = 0.5)/100
df = data.frame(IQ, workingMemory)
plot(df$workingMemory~df$IQ, xlab="IQ" , ylab="working memory", pch = 16, main = "working memory and intelligence")
cor.test(df$IQ, df$workingMemory)
```


##Warnings about correlation
But we have to remember that correlation does not imply causation. In our example, working memory does not cause IQ and vice versa, rather there are lots of intervening variables. 

The magnitude of a correlation is influenced by many factors, including: sampling (random and representative?), and the measurement of X & Y (are your measures of IQ reliable?). 

When you fail to get a representative sample you can get attenutation of correlation due to a restriction of range in one of your variables. For instance, if you only select college graduates, you have preselected for higher IQ and this can reduce the correlation.

This restriction of range essentially restricts variance ultimately impacting our ability to discern covariance. In the following scatterplot and correlation measure you can see this effect.

```{r}
dfAttenuated <- df[df$IQ >110, ]
plot(dfAttenuated$workingMemory~dfAttenuated$IQ, xlab="IQ" , ylab="working memory", pch = 16, main = "working memory and intelligence")
cor.test(dfAttenuated$IQ, dfAttenuated$workingMemory)
```

Finally, a correlation coefficient is a sample statistic just like the mean and won't be representative unless the correlation coefficient is 1. 

##Types of Correlation
There are several types of correlation coefficients, for different variable types. 

###The Pearson product-moment correlation coefficient (r)

This is used when both variables, X & Y, are continuous. 

###The Point bi-serial correlation

This is used when 1 variable is continuous and 1 is dichotomous. 

###The Phi coefficient
When both variables are dichotomous 

###Spearman rank correlation
When both variables are ordinal (ranked data)

## Focus on Pearson correlation
r = the degree to which X and Y vary together, relative to the degree to which X and Y vary independently. 

r = (covariance of X & Y)/(variance of X & Y)

There are a number of ways to calculate r e.g. 

the raw score formula and

the Z-score formula

Remember from our calculation for variance

$variance = SD^2 = MS = (SS/N)$

To calculate SS:

For each row, calculate the deviation score

$(X-M_x)$

Square the deviation scores

$(X-M_x)^2$

Sum the squared deviation scores

$SS_x = \Sigma[(X-M_x)^2]=\Sigma[(X-M_x)*(X-M_x)]$

##Sum of Cross Products
We need to calculate the sum of cross products (SP) to get r for our correlation 

for each row, calculate the deviation score on X
$(X-M_x)$

For each row, calculate the deviation score on Y
$(X-M_y)$

Then, for each row, multiply the deviation score on X by the deviation score on Y

$(X-M_x) * (Y-M_y)$

Then sum the "cross products"

$SP = \Sigma[(X-M_x) * (Y-M_y)]$

##Covariance
Covariance measures the relationship between two variables. 

Covariance = COV = SP/N

The formula for covariance is: 
$cov(X,Y) = \frac {\Sigma(X-M_x)(Y-M_y)}{N}$

or for inferential statistics where the denominator becomes n-1

$cov(X,Y) = \frac {\Sigma(X-M_x)(Y-M_y)}{N-1}$

Covariance is not scaled, so it can't tell you the strength of that relationship. To account for this, correlation takes covariance and scales it by the product of the standard deviations of the two variables.

```{r}
cov(df$IQ,df$workingMemory) # covariance score
cor.test(df$IQ,df$workingMemory) # correlation score
```


##Raw score formula
The raw score formula is thus:

$r = \frac {SP_{xy}}  {\sqrt{(SS_x * SS_y)}}$

remember this r value is the degree to which X and Y vary together, relative to the degree to which X and Y vary independently. 

In longer form: 

$SP = \Sigma[(X-M_x) * (Y-M_y)]$

$SS_x = \Sigma[(X-M_x)^2]=\Sigma[(X-M_x)*(X-M_x)]$

$SS_y = \Sigma[(Y-M_y)^2]=\Sigma[(Y-M_y)*(Y-M_y)]$

So the raw score formula to calculate the correlation coefficient r can be written out in two ways:

$r = \frac {SP_{xy}}  {\sqrt{(SS_x * SS_y)}}$

or,

$r = \frac{\Sigma[(X-M_x) * (Y-M_y)]} { \sqrt{(\Sigma(X-M_x)^2 * \Sigma(Y-M_y)^2)}}$

##Z-score formula
is the sum of the product of the Z-scores divided by N

$r = \frac{\Sigma(Z_x * Z_y)} {N}$

first we need to calculate the Z-scores

$Z_x = \frac{(X-M_x)}{SD_x}$

$Z_y = \frac{(Y-M_y)}{SD_y}$

where

$SD_x = \sqrt \frac{(\Sigma(X-M_x)^2}{N}$

$SD_y = \sqrt \frac{(\Sigma(Y-M_y)^2}{N}$

###Proof of equivalence
here the denominator is the standard deviation

$Z_x= \frac{(X-M_x)}  { \sqrt \frac{(\Sigma(X-M_x)^2}{N} }$

$Z_y= \frac{(Y-M_y)}  { \sqrt \frac{(\Sigma(Y-M_y)^2}{N} }$

###unpacked to it's full long form
here we have $Z_x$ multiplied by $Z_y$ divided by N

\[\begin{array}{lcc} r = \frac {\frac{(X-M_x)}  { \sqrt \frac{(\Sigma(X-M_x)^2}{N} } * \frac{(Y-M_y)}  { \sqrt \frac{(\Sigma(Y-M_y)^2}{N}}}{N}
\end{array}\]



###we can pack all this back together using some algebra
$r = \frac{\Sigma[(X-M_x) * (Y-M_y)]} { \sqrt{(\Sigma(X-M_x)^2 * \Sigma(Y-M_y)^2)}}$

which can be simplified further to:

$r = \frac {SP_{xy}}  {\sqrt{(SS_x * SS_y)}}$

which is the raw score formula.

##Variance and covariance
Variance = MS = SS/N

Covariance = COV = SP/N

Correlation is standardised covariance

it's standardised so the value is in the range -1 to +1

###Note on the denominators
Correlation for descriptive statistics

Divide by N

Correlation for inferential statistics

Divide by N-1

#Assumptions of Correlation
let's consider Pearson correlation

Assumptions when interpreting r:

Normal distributions for X and Y 

- how to detect violations? 

Plot histograms and examine summary stats

Linear relationship between X and Y

- how to detect violations? 

Examine scatterplots

Homoscedasticity

- how to detect violations? 

Examine scatterplots

Reliability of X and Y 

Validity of X and Y 

Random and representative sampling 

###Homoscedasticity and heteroscedasticity
In a scatterplot the vertical distance between a dot and the regression line reflects the amount of predicition error (known as the "residual")

The idea of Homoscedasticity is that those residuals are not related to X. The residuals should be chance errors and not systematic. 

If the residuals are related to X then we suspect some sort of confound in our study. This is termed Heteroscedasticity. 

A classic example of heteroscedasticity is that of income versus expenditure on meals. As one's income increases, the variability of food consumption will increase. A poorer person will spend a rather constant amount by always eating inexpensive food; a wealthier person may occasionally buy inexpensive food and at other times eat expensive meals. Those with higher incomes display a greater variability of food consumption.

```{r, echo=FALSE}
set.seed(123456)
x = rnorm(500,40000,10000)
b0 = 1 # intercept chosen at your choice
b1 = 1 # coef chosen at your choice
h = function(x) 1+.4*x # h performs heteroscedasticity function (here I used a linear one)
eps = rnorm(500,0,h(x))
y = b0 + b1*x + eps
plot(x,y)
#abline(lsfit(x,y))
#abline(b0,b1,col=2)

```


###Anscombe's Quartet
Why it's critical to look at your scatterplots.

Four datasets where the correlation is exactly the same.
The datasets also have the same variance. But clearly there are differences in these datasets. 


```{r, echo=FALSE}
require(stats); require(graphics)
summary(anscombe)

##-- now some "magic" to do the 4 regressions in a loop:
#ff <- y ~ x
#mods <- setNames(as.list(1:4), paste0("lm", 1:4))
#for(i in 1:4) {
#  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  ## or   ff[[2]] <- as.name(paste0("y", i))
  ##      ff[[3]] <- as.name(paste0("x", i))
#  mods[[i]] <- lmi <- lm(ff, data = anscombe)
#  print(anova(lmi))
#}

## See how close they are (numerically!)
#sapply(mods, coef)
#lapply(mods, function(fm) coef(summary(fm)))

## Now, do what you should have done in the first place: PLOTS
#op <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))
#for(i in 1:4) {
#  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
#  plot(ff, data = anscombe, col = "red", pch = 21, bg = "orange", cex = 1.2,
#       xlim = c(3, 19), ylim = c(3, 13))
#  abline(mods[[i]], col = "blue")
#}
#mtext("Anscombe's 4 Regression data sets", outer = TRUE, cex = 1.5)
#par(op)
```



```{r, echo=FALSE}
require("ggplot2")
require("gridExtra")

cor1 <- format(cor(anscombe$x1, anscombe$y1), digits=2)
cor2 <- format(cor(anscombe$x2, anscombe$y2), digits=2)
cor3 <- format(cor(anscombe$x3, anscombe$y3), digits=2)
cor4 <- format(cor(anscombe$x4, anscombe$y4), digits=2)
 
line1 <- lm(y1 ~ x1, data=anscombe)
line2 <- lm(y2 ~ x2, data=anscombe)
line3 <- lm(y3 ~ x3, data=anscombe)
line4 <- lm(y4 ~ x4, data=anscombe)
 
circle.size = 5
colors = list('red', '#0066CC', '#4BB14B', '#FCE638')
 
plot1 <- ggplot(anscombe, aes(x=x1, y=y1)) + geom_point(size=circle.size, pch=21, fill=colors[[1]]) +
  geom_abline(intercept=line1$coefficients[1], slope=line1$coefficients[2]) +
  annotate("text", x = 12, y = 5, label = paste("correlation = ", cor1))
 
plot2 <- ggplot(anscombe, aes(x=x2, y=y2)) + geom_point(size=circle.size, pch=21, fill=colors[[2]]) +
  geom_abline(intercept=line2$coefficients[1], slope=line2$coefficients[2]) +
  annotate("text", x = 12, y = 3, label = paste("correlation = ", cor2))
 
plot3 <- ggplot(anscombe, aes(x=x3, y=y3)) + geom_point(size=circle.size, pch=21, fill=colors[[3]]) +
  geom_abline(intercept=line3$coefficients[1], slope=line3$coefficients[2]) +
  annotate("text", x = 12, y = 6, label = paste("correlation = ", cor3))
 
plot4 <- ggplot(anscombe, aes(x=x4, y=y4)) + geom_point(size=circle.size, pch=21, fill=colors[[4]]) +
  geom_abline(intercept=line4$coefficients[1], slope=line4$coefficients[2]) +
  annotate("text", x = 15, y = 6, label = paste("correlation = ", cor4))
 
grid.arrange(plot1, plot2, plot3, plot4, top='Anscombe Quadrant -- Correlation Demostration')
```

#Measurement
##Reliability - do we have reliable measurements? 

If I step on a scale multiple times do I get the same weight?

But some values are harder to evaluate for reliability. Raw scores are imperfect, e.g. body temperature is suscpetible to systematic bias and chance error. 

Classical test theory states that, in a perfect world, it would be possible to obtain a "true score" rather than a "raw score" (X) 

X = true score + bias + error 

A measure (X) is considered reliable as it approaches the true score

The problem is we don't know the true score so we estimate reliability

###Methods to estimate reliability 
Test/re-test

Parallel tests

Inter-item reliability

####Example of body temperature
Can be measured in 3 ways,

Orally, internally, IR wand. 

The IR wand has a systematic bias in that it always tends to record a higher temperature. 

```{r, echo=FALSE}
x<-rnorm(1000, mean = 37, sd = 1)
hist(x, xlab = "temperature in celsius",
     main = "Orally measured body temperature")
```
```{r, echo=FALSE}
x<-rnorm(1000, mean = 38, sd = 1)
hist(x, xlab = "temperature in celsius",
     main = "IR measured body temperature")
```

###Test/re-test
One way to get a reliability estimate

Measure everyone twice so we'll have data for X1 and X2

Should be a strong correlation between the two measures otherwise you don't have a reliable measure

However, if the bias is uniform then we won't detect it with the test/re-test method. So in the case of the IR thermometer reading a little high, test/re-test won't work. The correlation will be high even though there is a bias.  

###Parallel tests
Measure body temp with the wand (X1) and with the oral thermometer(X2)

The correlation betweeen X1 and X2 is an estimate of reliability.

AND, now the bias of the wand will be revealed because you have two tests rather than one.  

###Inter-item
the most commonly used method in the social sciences because the focus is usually on human subjects who are difficult to work with. 

Test/re-test and parallel tests are time consuming

Inter-item is therefore more cost efficient. 

For example, suppose a 20 item survey is designed to measure extraversion 

Randomly select 10 items to get sub-set A (X1)

The other items become sub-set B (X2)

Now we have two assessments of extraversion built into one overall survey. 

If they're all getting at one personality trait then there should be a correlation between X1 and X2 which would represent an estimate of reliability. 
#Measurement
##Validity 
What is a construct? 

An ideal "object" that is not directly observable as opposed to "real" observable objects

For example, "intelligence" is a construct. 

###How do we operationalise a construct? 
The process of defining a construct to make it observable and quantifiable e.g. intelligence tests. 

###Construct validity 
How do we assess the validity of a construct? 

Let's take an example of a construct: verbal ability in children

We might operationalise this construct by using a vocabulary test.

####Content validity 
In the case of the vocabulary test we would ask does the test consist of words that children in the population and sample know? 

####Convergent validity 
Does the test correlate with other, established measures of verabal ability e.g. with reading comprehension.

####Divergent validity 
Does the test correlate less well with measures designed to test a different type of ability e.g. spatial ability, or even more extreme, the height of the student where there should very little correlation. 

####Nomological validity 
Are scores on the test consistent with more general theories, e.g. for child development and neuroscience. In that case a child with neural damage or disease to brain regions associated with language development should score lower on the test. 

#Sampling
Remember, we want our sample to be random and representative. 

Let's set up a die and roll it a 1000 times, then plot the results on a histogram

```{r}
p.die <- rep(1/6,6)
sum(p.die)
die <- 1:6
s <- table(sample(die, size=1000, prob=p.die, replace=T))
lbls = sprintf("%0.1f%%", s/sum(s)*100)
barX <- barplot(s, ylim=c(0,200))
text(x=barX, y=s+10, label=lbls)

```

As expected each of the 6 sides came up approximately the same number of times.

We can do something similar but with a loaded die

```{r}
p.die <- c(0.25,0.15,0.15,0.15,0.15,0.15)
sum(p.die)
die <- 1:6
s <- table(sample(die, size=1000, prob=p.die, replace=T))
lbls = sprintf("%0.1f%%", s/sum(s)*100)
barX <- barplot(s, ylim=c(0,300))
text(x=barX, y=s+10, label=lbls)

```

##Sampling error 
The difference between the population and the sample.

This is important because we can't get everyone/everything in the popualtion.  

Notice that the random histogram is not perfectly random

There is some fluctuation due to sampling error. 

##Problem
We don't know the population parameters 

So, how do we estimate sampling error?

##Estimating sampling error 
Sampling error mainly depends on the size of the sample, relative to the size of the population. 

as sample size increases, sampling error decreases.

It also depends on the variance in the population (which we don't know).

as variance increases, sampling error increases. 

The following histogram shows how the distribution becomes more uniform as the sample size increases illustrating the effect of sample size on sampling error. 

```{r,echo=FALSE}
#p.die <- rep(1/6,6)
#sum(p.die)
#die <- 1:6
#par(mfrow=c(2,3))

#for(x in seq(10,110,20)) {
#  s <- table(sample(die, size=x, prob=p.die, replace=T))
  #lbls = sprintf("%0.1f%%", s/sum(s)*100)
#  barX <- barplot(s)#ylim=c(0,200))
#  main=x
 # text(x=barX, y=s+10)#, label=lbls)
#}

par(mfrow=c(2,3))
for(i in 1:6){
x <- runif(10**i)
hist(x,prob=TRUE, col="grey",ylim=c(0,2),main = paste(10**i," Draws"))
curve(dunif(x),add=TRUE,col="red",lwd=2)}


```


We can check out the same effect by sampling from the normal distribtion using increasing sample sizes. As we can see the distribution becomes more normal as the sample size goes up. 

```{r, echo=FALSE}
#par(mfrow=c(2,3))
#for(x in seq(10,1000,175)) {
#  normDist<-rnorm(x,mean=0,sd = 1)
#  hist(normDist,main="")
#}

#Normal Sampling
par(mfrow=c(2,3))
for(i in 1:6){
x <- rnorm(10**i)
hist(x,prob=TRUE,col="grey",ylim=c(0,.6),xlim=c(-4,4),main=paste(10**i," Draws"))
curve(dnorm(x),add=TRUE,col="red",lwd=2)}
```

###Estimating sampling error 
Sampling error is estimated from the size of the sample and the variance in the sample. This is under the assumption that the sample is random and representative of the population. 

###Standard error
Standard error is an estimate of the average amount of sampling error

$SE = \frac{SD}{\sqrt{N}}$

SE = standard error

SD = standard deviation of the sample 

N = Size of the sample 

We can see from here that as the sample size in the denominator increases then the standard error is going to rise. 

By contrast, as the standard deviation increases (which should reflect the population standard deviation) so too will the standard error. 

Here's an example of taking a sample from a population that is not very variable. Note the low standard error. 
```{r}
population<-rnorm(1000,mean=0,sd=1)
sd(population)
sampleDraw<-sample(population,10)
sd(sampleDraw)
length(sampleDraw)
sd(sampleDraw)/sqrt(length(sampleDraw))
par(mfrow=c(1,2))
hist(sampleDraw)
hist(population)
```

Here's an example of taking a sample from a population that is variable. Note the higher standard error. 

```{r}
population<-rnorm(1000,mean=0,sd=10)
sd(population)
sampleDraw<-sample(population,10)
sd(sampleDraw)
length(sampleDraw)
sd(sampleDraw)/sqrt(length(sampleDraw))
par(mfrow=c(1,2))
hist(sampleDraw)
hist(population)
```

#Regression
A regression is a statistical analysis used to predict scores on an outcome variable, based on scores on one or multiple predictor variables

simple regression: one predictor variable 

multiple regression: multiple predictor variables

##Regression equation
$Y = m + bX + e$
this is the equation of a line 

Y = a linear function of X
m = intercept
b = slope
e = residual error

other notation, more commonly used for statistics
$Y = B_0 +B_1X_1 + e$

Y = a linear function of $X_1$
$B-0$ = intercept = regression constant 
$B_1$ = slope = regression coefficient 
e = residual error

##Model R and R^2
R = multiple correlation coefficient

R = $r_{y'y}$

The correlation between the predicted scores and the observed scores 

$R^2$ the percentage of variance in Y explained by the model.

###Simple regression example

```{r}
set.seed(20)
IQ <- rnorm(250, mean = 100, sd = 10)
workingMemory <- IQ*rnorm(250, mean = 3, sd = 0.5)/100
df = data.frame(IQ, workingMemory)
plot(df$workingMemory~df$IQ, xlab="IQ" , ylab="working memory", pch = 16, main = "working memory and intelligence")
cor.test(df$IQ, df$workingMemory)
model1 <- lm(df$workingMemory~df$IQ)
abline(model1)
summary(model1)
```

The estimate of the slope is 0.02599, so with every one unit increase in X there is a 0.02599 increase in Y. 

The estimate of the intercept is 0.38105, the predicted score on Y when X = 0. 

Thus the regression equation is 

$Y=0.38105+0.02599(X)$

In a simple regression the $R^2$ value 0.2323 is equal to the correlation coefficient 0.4819546 to the power of 2.

```{r}
0.4819546^2
```

The goal with regression is to produce better models so we can generate more accurate predictions

Add more predictor variables and/or

develop better predictor variables.

###Multiple Regression
Add in another predictor variables

$Y=B_0 +B_1Y_1 + B_2Y_2 + e$

Now we need to solve for $B_0$ & $B_1$ & $B_2$


```{r}
set.seed(20)
IQ <- rnorm(250, mean = 100, sd = 10)
workingMemory <- IQ*rnorm(250, mean = 3, sd = 0.5)/100
salary <- IQ*rnorm(250, mean = 500, sd = 0.1)
df = data.frame(IQ, workingMemory, salary)
model2 <- lm(df$workingMemory~df$IQ+df$salary)
summary(model2)
```

The linear combination of two predictors can do better at predicting the outcome than any one predictor by itself. 

##Calculation of regression coefficients
Regression equation
$Y = B_0 + B_1X_1 + e$

$\hat{Y} = B_0 + B_1X_1$ where $\hat{Y}$ is the predicted score for Y

$Y-\hat{Y} = e(residual)$

The values of the coefficients (e.g. $B_1$) are estimated such that the regression model yields optimal predictions. What we want to do is minimise the residuals i.e. minimise the prediction error.

```{r}
x<- c(2000,2001,2002,2003,2004,2005)
y<-c(9.34,8.50,7.62,6.93,6.60,7.2)
m1<-lm(y~x)
fitted<-predict(lm(y~x))
plot(x,y, pch =16, xlim = c(2000,2005), ylim = c(5,10))
abline(m1)
for (i in 1:6) lines(c(x[i],x[i]),c(y[i],fitted[i]))

```


###ordinary least squares estimation 
Minimise the sum of the squared (SS) residuals

SS.Residual = $\Sigma(Y-\hat{Y})^2$

The best fit slope is found by rotating the line until the SS.Residual is minimised. This gets the maximum likelihood estimate of the slope.  

###Visual approach 
We have the sum of squared deviation scores (SS) in variable Y = SS.Y 

We also have the sum of squared deviation scores (SS) in variable Y = SS.Y

The overlap between these two is the sum of cross products between X and Y  i.e. SP.XY. So the degree to which the two variables correlate will be a measure of how much overlap there is between the two.

Here's an example with low overlap and thus low correlation.

```{r, echo=FALSE}
library(VennDiagram)
draw.pairwise.venn(area1 = 15, area2 = 15, cross.area = 2, category = c("SS.X", 
    "SS.Y"))
```

Here's an example with high overlap and thus high correlation.

```{r, echo=FALSE}
library(VennDiagram)
draw.pairwise.venn(area1 = 15, area2 = 15, cross.area = 10, category = c("SS.X", 
    "SS.Y"))
```
SP.XY can be thought of as the sum of squares of the model i.e. sum of cross products = SS of the model = SP.XY = SS.Model

Some of the variance in Y is explained by the model and some of it is unexplained, that's the residual. SS.Residual = (SS.Y - SS.Model)

###Formula for the unstandardised coefficient
In a simple linear regression 

$B_1 = r * (\frac{SD_y}{SD_x})$ 

where r is the correlation coefficient.

We divide by the standard deviations because we need to take into account the scale of Y and the scale of X. Y may be much more variable than X for instance so this division deals with that.  

###Formula for the standardised coefficient
Where everything is in Z-scores

$SD_y = SD_x = 1$

$B=r*(\frac{SD_y}{SD_x})$

$\beta=r$

This is only true for simple linear regression.

```{r}
x<- c(2000,2001,2002,2003,2004,2005)
y<-c(9.34,8.50,7.62,6.93,6.60,7.2)
plot(x,y, pch = 16)
# take the Z scores for x and y
Zx <- (x-mean(x))/sd(x)
Zy <- (y-mean(y))/sd(y)

# get the correlation for X and Y
cor(Zx,Zy)
m1<-lm(Zy~Zx)
# Get the slope of y as a function of x
coef(m1)[2]

# also recall from earlier that the R squared value of a simple linear regression is equal to the correlation coefficient squared. 
summary(m1)$r.squared
cor(Zx,Zy)^2
```


The correlation gives you a bounded measurement that can be interpreted independently of the scale of the two variables. The closer the estimated correlation is to $\pm 1$, the closer the two are to a perfect linear relationship. The regression slope, in isolation, does not tell you that piece of information.

The regression slope gives a useful quantity interpreted as the estimated change in the expected value of Y for a given value of X. Specifically, $\hat{\beta}$ tells you the change in the expected value of Y corresponding to a 1-unit increase in X. This information can not be deduced from the correlation coefficient alone.

#Assumptions of linear regression
Normally distributed residuals 

Linear relationship between X and Y

Homoscedasticity

##Return to Anscombe's Quartet
The following graphs and their regression coefficients show how similar the four data sets are even for linear regression models.The regression equation for each  is approximately:

$\hat{Y}=3+0.5(X_1)$

```{r, echo=FALSE}
require(stats); require(graphics)
#summary(anscombe)

##-- now some "magic" to do the 4 regressions in a loop:
ff <- y ~ x
mods <- setNames(as.list(1:4), paste0("lm", 1:4))
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  ## or   ff[[2]] <- as.name(paste0("y", i))
  ##      ff[[3]] <- as.name(paste0("x", i))
  mods[[i]] <- lmi <- lm(ff, data = anscombe)
 # print(anova(lmi))
}

# See how close they are (numerically!)
sapply(mods, coef)
#lapply(mods, function(fm) coef(summary(fm)))

## Now, do what you should have done in the first place: PLOTS
op <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  plot(ff, data = anscombe, col = "red", pch = 21, bg = "orange", cex = 1.2,
       xlim = c(3, 19), ylim = c(3, 13))
  abline(mods[[i]], col = "blue")
}
mtext("Anscombe's 4 Regression data sets", outer = TRUE, cex = 1.5)
par(op)
```

But only the top left panel looks suitable for a linear regression model. The others look like they don't satisfy the various assumptions of linear regression. 

In order to test this we can save the residuals of each model. 

$Y=B_0+B_1+e$

where

$e=(Y-\hat{Y})$

We can then look at the residuals as a function of the X predictor variable. Then we examine a scatterplot with the X variable on the X-axis and the residuals on the Y-axis.

```{r, echo=FALSE}
# Set to put all four in one plot
# This is optional, but makes it easier to compare
#par( mfrow = c(2,2))


# Set margins to make them fit better
#par( mar = c(5, 4, 1, 2) + 0.1)

# Plot them
#plot(y1 ~ x1, data = anscombe)
#abline(ansA, col = "red3",lwd = 2)

#plot(y2 ~ x2, data = anscombe)
#abline(ansB, col = "red3",lwd = 2)

#plot(y3 ~ x3, data = anscombe)
#abline(ansC, col = "red3",lwd = 2)

#plot(y4 ~ x4, data = anscombe)
#abline(ansD, col = "red3",lwd = 2)
```

```{r, echo=FALSE}
# Set to put all four in one plot

ansA <- lm(y1 ~ x1, data = anscombe)
ansB <- lm(y2 ~ x2, data = anscombe)
ansC <- lm(y3 ~ x3, data = anscombe)
ansD <- lm(y4 ~ x4, data = anscombe)

par( mfrow = c(2,2))

# Set margins to make them fit better
par( mar = c(5, 4, 1, 2) + 0.1)

# Plot the residutals
plot( resid(ansA) ~ anscombe$x1 )
plot( resid(ansB) ~ anscombe$x2 )
plot( resid(ansC) ~ anscombe$x3 )
plot( resid(ansD) ~ anscombe$x4 )
```

The plot in the top left is what we are looking for. We don't want any pattern in our residual plot and this suggests no systematic error. The other plots suggest there is a relationship between X and the residual. 

#Null Hypothesis Significance Testing (NHST)
NHST is a procedure for hypothesis testing. 
We can consider NHST as a game where step 1 is the identification of the nully hypothesis and the alternative hypothesis. 

###Step 1
for a correlational study

$H_0$ = null hypothesis, e.g. r = 0 

$H_A$ = alternative hypothesis, e.g. r > 0

where r is the correlation coefficient

or for a regression model

$H_0$ = null hypothesis, e.g. B = 0 

$H_A$ = alternative hypothesis, e.g. B > 0

where B is the slop of the regression model

If the alternative hypothesis predicts the direction of the relationship between X & Y (positive Vs negative) it is termed a directional test (aka a one-tailed test)

Alternatively we could be agnositc and not have any idea about the direction of the relationship. In this case it would be a non-directional test (aka a two-tailed test)

The non-directional test for a regression model would be set up like this: 

$H_0$ = null hypothesis, e.g. B = 0 

$H_A$ = alternative hypothesis, e.g. B != 0

###Step 2
Assume $H_0$ is true, then calculate the probability of observing data with these characteristics, given that $H_0$ is true. This can be confusing because it's the opposite way you'd approach a study. For instance, Jonas Salk didn't predict his vaccination would have no effect.

$p=P(D|H_0)$

The probability of the data given the null hypothesis is true. This is the p-value. If the p-value is very low, then reject $H_0$, else retain $H_0$

#4 possible outcomes of NHST
Either the null is true or it's false and then, as scientists, we have to successfully pick this out.
```{r, echo=FALSE}
NHST <- matrix(c("Correct decision","Type 1 error (false alarm)","Type 2 error (miss)","Correct decision"),ncol=2,nrow=2)
 colnames(NHST) <- c("Retain Null","Reject Null")
 rownames(NHST) <- c("Null is true","Null is false")
 NHST <- as.table(NHST)
 NHST
```

![](C:\\Users\\akane\\Desktop\\Science\\Teaching\\R-Course-UCC\\NHST.jpg)

##NHST Overview
$p=P(D|H_0)$

Given that the null hypothesis is true, the probability of these, or more extreme data, is p. 

This does not mean the probability of the null hypothesis being true in p. 

In other words, $P(D|H_0)!=P(H_0|D)$

##NHST so far 

for correlation
- is the correlation significantly different from zero?

B
- is the slope of the regression line for X significantly different from zero?

##NHST for B - the slope of the regression line
t = B/SE

B = the unstandardised regression coefficient

SE = standard error

SE = $\sqrt{\frac{SS.Residual}{N-2}}$ 

#NHST Problems and Remedies

###1 - Biased by Sample Size
The p-value you get is based on a t-value and this t -value is affected by the standard error. N is in the denominator of the standard error and standard error is in the denominator of the t-value. Thus, if sample size goes up the standard error will go down, and if the standard error goes down the t-value will go up regardless of the slope value.

t = B/SE

B = the unstandardised regression coefficient

SE = standard error

SE = $\sqrt{\frac{SS.Residual}{N-2}}$ 

```{r}
x<-rnorm(10000,mean=100,sd=10)
y<-rnorm(10000,mean=100,sd=10)*x
df<-data.frame (x,y)
smallSample<-df[sample(nrow(df), 10), ]
m1<-lm(smallSample$y~smallSample$x)
pVal <- anova(m1)$'Pr(>F)'[1];pVal


bigSample<-df[sample(nrow(df), 100), ]
m2<-lm(bigSample$y~bigSample$x)
pVal <- anova(m2)$'Pr(>F)'[1];pVal
```
###2 - Arbitrary Decision Rule
The cut-off value (alpha) is arbitrary

$p < 0.05$ is considered standard but still arbitrary. 

Problems arise when p is close to 0.05 but not less than 0.05. p-hacking etc. 

###3 - Yokel local test
Many researchers use NHST because it#s the only approach they know. NHST encourages weak hypothesis testing. 

###4 - Error prone
Type 1 errors
- The probability of Type 1 errors increases when researchers conduct multiple NHSTs, especially when it's on the same dataset. Have to correct for these multiple tests. 

Type 2 errors
- Many fields of research are plagued by a large degree of sampling error because we can only get a relatively small sample relative to the population, which makes it difficult to detect an effect, even when the effect exists.

###Shady Logic
Modus tollens operates like this:

If p then q

Not q

Therefore, not p

Equivalently, in the language of statistics:

If the null hypothesis is correct, then these data can not occur

The data have occurred 

Therefore, the null hypothesis is false

But in NHST the language is more probabilistic than this:

If the null is correct, then these data are highly unlikely.

These data have occurred

Therefore, the null is highly unlikely 

To take an equivalent example:

If a person plays football, then he or she is probably not a professional player

This person is a professional player

Therefore, he or she probably does not play football.

##NHST Remedies
###Remedy for Bias by sample size
Supplement all NHSTs with estimates of effect size to get at the magnitude of the effect. For example, in regression, report standardised regression coefficients and the model R-squared.

###Remedy for Arbitrary decision rule
Again supplement all NHSTs with estimates of effect size to get at the magnitude of the effect. Also, avoid phrases such as "marginally signifcant" or "highly significant". 

###Remedy for Yokel local test
Learn other forms of hypothesis testing. Consider multiple alternative tests and use model selection. 

###Remedy for NHST being error prone
Replicate significant effects to avoid long-term impact of type 1 errors

Obtain large and representative samples to avoid type 2 errors. 

###Remedy for Shady logic
Simply remember, $p=P(D|H_0)$

Or avoid NHST, and instead,

Report confidence intervals only or, 

Apply Bayesian inference

#Central Limit Theorem
###Review of histograms 
Histograms are used to display distributions e.g. the body temperature of a random sample of healthy people.

```{r}
x <- rnorm(100, mean = 37,sd=1)
Zx <- (x-mean(x))/sd(x)
hist(x, main = "normal histogram of body temp in Celsius")
hist(Zx, main = "normal histogram of Z score body temp")
```

If a distribution is perfectly normal then the properties of the distribution are known.

```{r}
x<-seq(-3,3,length=200)
s = 1
mu = 0
y <- (1/(s * sqrt(2*pi))) * exp(-((x-mu)^2)/(2*s^2))
plot(x,y, type="l", lwd=2, col = "black", xlim = c(-3.5,3.5))
```

Things to note about a normal distribution.

50% of the data fall above the mean and 50% below. 

The majority of the data fall between 2 standard deviations above and below the mean. If you are higher or lower than these values then you are an extreme point.

This allows for predictions about the distribution because we know predictions aren't certain rather they are probabilistic. 

For example if one person is randomly selected from the sample, what is the probability that his or her body temperature is less than Z = 0 (or X = 37 for degrees celsius)? It's, p = 0.50. 

```{r}
x=seq(-3,3,length=200)
y=dnorm(x,mean=0,sd=1)
plot(x,y,type="l")
x=seq(-3,0,length=100)
y=dnorm(x,mean=0,sd=1)
polygon(c(-3,x,0),c(0,y,0),col="skyblue")
```

Alternatively, if one person is randomly selected from the sample, what is the probability that his or her body temperature is greater than Z = 2 (or X = 38 for degrees celsius)? It's, p = 0.20. 

```{r}
x=seq(-3,3,length=200)
y=dnorm(x,mean=0,sd=1)
plot(x,y,type="l")
x=seq(2,3,length=100)
y=dnorm(x,mean=0,sd=1)
polygon(c(2,x,3),c(0,y,0),col="skyblue")
```

If this sample is healthy, then no one should have a fever. 

I detected a  person with a fever

Therefore, this sample is not 100% healthy. 

##Sampling Distributions 
But rather than using NHST on distributions of individuals we use NHST on distributions of sample statistics.

A sampling Distributions is a distribution of sample statistics, obtained from multiple samples, for example, a distribution of sample means, of sample correlations, of sample regression coefficients. 

Sampling distributions are important in statistics because they provide a major simplification on the route to statistical inference. More specifically, they allow analytical considerations to be based on the sampling distribution of a statistic, rather than on the joint probability distribution of all the individual sample values.

It's important to realise that a sampling distribution is  hypothetical. Instead we will only have a single sample.

Let's assume a mean is calculated from a sample, obtained randomly from the population. 

Assume a certain sample size, N

Now assume we had multiple random samples (which is not what we do in practice), all of size N, and therefore many sample means. These would all differ a little bit because of sampling error.

Collectively, they form a sampling distribution.

##Marrying sampling distributions and probability

If one sample is obtained from a normal healthy population, what is the probability that the sample mean is less than Z = 0? Again, it's p = 0.50. 

If one sample is obtained from a normal healthy population, what is the probability that the sample mean is less than Z = 2? Again, it's p = 0.20. 

In the latter case, if this population is healthy, then no one sample should have a high mean body temperature.

I obtained a very high sample mean.

Therefore, the population is not healthy.

#Central Limit Theorem
Three principles

1. The mean of a sampling distribution is the same as the mean of the population

2. The standard deviation of the sampling distribution is the square root of the variance of the sampling distribution $\sigma^2 = \frac{\sigma^2}{N}$

3. The shape of a sampling distribution is approximately normal if either (a) N >= 30 or (b) the shape of the population is normal. 

##NHST and the Central Limit Theorem
###Multiple regression

Assume the null is true

Conduct a study

Calculate B, SE and t

where t = B/SE

the p-value is a function of t and sample size

Conceptually, the t-value is a ratio of what we observed (e.g. the slope of the regression line) relative to what we would expect due to chance (e.g. the slope is zero). A ratio of 1 would be something around the null. 

If the null hypothesis is true, then no one sample should have a very low or very high slope. Thus, if I obtain a very high slope I should reject the null. But what does 'very high' mean? 

The very low or very high values depend on the normal distribution. Remember, the shape of a sampling distribution is approximately normal if either (a) N >= 30 or (b) the shape of the population is normal. 

That means I can make probability judgements about the outcome. Note, that the third principle of the central limit theorem didn't say you get a normal distribution, rather it said you approximate one. 

Instead, we get a t-distribution which comes from a family that are dependent on the sample size. As your sample size gets smaller your t-distribution gets a little wider which means you need a larger t-value to get out into the extremes to get a low p-value.

This all means that our ideas of 'very high' or 'very low' come from p being < 0.05.

Remember, that sampling error, and therefore standard error, is largely determined by sample size.

Standard error is the standard deviation of the sampling distribution. As samples get larger they're going to squeeze in around the mean and the standard error will decrease. It's important to note that NHST is biased by sample size. 

t = B/SE

B = the unstandardised regression coefficient

SE = standard error

SE = $\sqrt{\frac{SS.Residual}{N-2}}$ 

```{r, echo=FALSE}
# Display the Student's t distributions with various
# degrees of freedom and compare to the normal distribution

x <- seq(-4, 4, length=100)
hx <- dnorm(x)

degf <- c(1, 3, 8, 30)
colors <- c("red", "blue", "darkgreen", "gold", "black")
labels <- c("df=1", "df=3", "df=8", "df=30", "normal")

plot(x, hx, type="l", lty=2, xlab="x value",
  ylab="Density", main="Comparison of t Distributions")

for (i in 1:4){
  lines(x, dt(x,degf[i]), lwd=2, col=colors[i])
}

legend("topright", inset=.05, title="Distributions",
  labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors)
```

As sample size increases, the actual mean approximates zero and the standard error shrinks as a function of sample size. 

```{r}
meanSamplingDist <- vector(length=length(i))
SESamplingDist <- vector(length=length(i))
for(i in 1:6){
  x <- rnorm(10**i)
  Zx <- (x-mean(x)/sd(x))
  SE <- sd(Zx)/sqrt(length(Zx))
  meanSamplingDist[i]<-(mean(Zx))
  SESamplingDist[i]<- (SE)
}

meanSamplingDist # the mean of the sampling distribution
SESamplingDist # the standard error of the sampling distribution


```

#F-tests
F tests are most commonly used for two purposes:

1. in ANOVA, for testing equality of means (and various similar analyses); and

2. in testing equality of variances

```{r, echo=FALSE}

x1<-c(1:10)
y1<-c(1:10)
z1<-c(1:10)
data1<-c(x1,y1,z1)
groups1 = factor(rep(letters[1:3], each = 10))
fit1 = lm(formula = data1 ~ groups1)
# anova (fit1)
fstat <- summary(fit1)$fstatistic ; fstat
```

```{r, echo=FALSE}
x2<-c(1:10)
y2<-c(4:13)
z2<-c(1:10)
data2<-c(x2,y2,z2)
groups2 = factor(rep(letters[1:3], each = 10))
fit2 = lm(formula = data2 ~ groups2)
# anova (fit2)
fstat <- summary(fit2)$fstatistic ; fstat
```

```{r, echo=FALSE}
x3<-c(7:16)
y3<-c(4:13)
z3<-c(1:10)
data3<-c(x3,y3,z3)
groups3 = factor(rep(letters[1:3], each = 10))
fit3 = lm(formula = data3 ~ groups3)
# anova (fit3)
fstat <- summary(fit3)$fstatistic ; fstat
```

```{r, echo=FALSE}
x4<-c(19:28)
y4<-c(4:13)
z4<-c(1:10)
data4<-c(x4,y4,z4)
groups4 = factor(rep(letters[1:3], each = 10))
fit4 = lm(formula = data4 ~ groups4)
# anova (fit4)
fstat <- summary(fit4)$fstatistic ; fstat

```

```{r, echo=FALSE}
par( mfrow = c(2,2))
boxplot(x1,y1,z1, horizontal = TRUE, ylim=c(0,30))
boxplot(x2,y2,z2, horizontal = TRUE, ylim=c(0,30))
boxplot(x3,y3,z3, horizontal = TRUE, ylim=c(0,30))
boxplot(x4,y4,z4, horizontal = TRUE, ylim=c(0,30))

```

If the null hypothesis (equality of population means) were true, you'd expect some variation in sample means, and would typically expect to see F ratios roughly around 1. Smaller F statistics result from samples that are closer together than you'd typically expect, so you aren't going to conclude the population means differ.

That is, for ANOVA, you'll reject the hypothesis of equality of means when you get unusually large F-values and you won't reject the hypothesis of equality of means when you get unusually small values (it may indicate something, but not that the population means differ).

```{r}
x<-seq(0,5,0.01); plot(x,df(x,df1=5,df2=5), type="l")
```

```{r}
x<-seq(0,5,0.01); plot(x,df(x,df1=2,df2=27), type="l")
```

This illustration shows that we only want to reject when F is in its upper tail.

```{r, echo=FALSE}
fstat <- summary(fit4)$fstatistic

#library(HH)
old.omd <- par(omd=c(.05,.88, .05,1))
F.setup(df1=fstat['numdf'], df2=fstat['dendf'])
F.curve(df1=fstat['numdf'], df2=fstat['dendf'], col='skyblue')
F.observed(fstat['value'], df1=fstat['numdf'], df2=fstat['dendf'])
par(old.omd)
```

#Probability Density Functions 
The "bell curve" shape is governed by the PDF. However, the actual "y"-value of this curve is itself more or less meaningless. The integral of the PDF $f(x)$ gives the probability that your random variable is less than some value: 

$P(x<X) = \int_{\infty}^{X}f(x)dx$. 

This is known as the CDF, or cumulative distribution function. By the fundamental theorem of calculus, the PDF is then the derivative of the CDF; that is, the PDF is the derivative of a function that returns a probability. So what is that intuitively? Honestly... it's not really anything. The "units" of the vertical axis in the PDF plot don't lead to anything intuitive; they are meaningful, but only in a derived, mathematical sense.

The area under the pdf equals 1. If the pdf value $f(x)$ exceeds 1 for some and indeed many values of $x$, that is perfectly fine: but $f(x)$ cannot exceed 1 for all $x$ in an interval $I$ of length exceeding 1. If the latter condition were to hold, then:

$\int_{I}^{}f(x)dx$ = area under pdf in interval I > 1

in violation of the constraint that the total area is 1. The value of $f(x)$ is not a probability. The units of $f(x)$ are probability per unit length and you must multiply by length (more generally, find an area) to get a probability.

As a consequence, some people wish to think that $f(X)$ is the probability that $x=X$, but this is untrue for continuous distributions $(P(x=X)=0)$. However, for the PDF's discrete analog, the Probability Mass Function (PMF), this statement is quite true.

```{r,echo=FALSE}
set.seed(3000)
xseq<-seq(-4,4,.01)
densities<-dnorm(xseq, 0,1)
cumulative<-pnorm(xseq, 0, 1)
randomdeviates<-rnorm(1000,0,1)
 
par(mfrow=c(1,2), mar=c(3,4,4,2))

plot(xseq, densities, col="darkgreen",xlab="", ylab="Density", type="l",lwd=2, cex=2, main="PDF of Standard Normal", cex.axis=.8)

plot(xseq, cumulative, col="darkorange", xlab="", ylab="Cumulative Probability",type="l",lwd=2, cex=2, main="CDF of Standard Normal", cex.axis=.8)

#hist(randomdeviates, main="Random draws from Std Normal", cex.axis=.8, xlim=c(-4,4))
```



##Interactions
Interactions allow us assess the extent to which the association between one predictor and the outcome depends on a second predictor. 